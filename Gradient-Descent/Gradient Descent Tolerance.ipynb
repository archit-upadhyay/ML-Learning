{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNxtMv0cdGXR",
        "outputId": "bb517c61-8b9d-46af-eaf8-807d9a4dead5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value of x: 70\n",
            "[56.4, 45.519999999999996, 36.815999999999995, 29.852799999999995, 24.282239999999994, 19.825791999999996, 16.2606336, 13.40850688, 11.126805504, 9.3014444032, 7.841155522559999, 6.672924418048, 5.7383395344384, 4.99067162755072, 4.392537302040576, 3.9140298416324604, 3.531223873305968, 3.2249790986447744, 2.9799832789158196, 2.7839866231326558, 2.6271892985061247, 2.5017514388048996, 2.4014011510439195, 2.3211209208351358]\n",
            "[13.600000000000001, 10.88, 8.703999999999999, 6.9632, 5.5705599999999995, 4.456447999999999, 3.5651583999999996, 2.85212672, 2.281701376, 1.8253611008000001, 1.46028888064, 1.168231104512, 0.9345848836096, 0.74766790688768, 0.598134325510144, 0.47850746040811515, 0.3828059683264921, 0.30624477466119365, 0.2449958197289549, 0.19599665578316394, 0.15679732462653118, 0.12543785970122495, 0.10035028776097993, 0.0802802302087839]\n",
            "24\n"
          ]
        }
      ],
      "source": [
        "## Gradient Descent Algo with Tolerance - Question 2 Trial\n",
        "import nump as np\n",
        "def cost_func(x):\n",
        "  return ((x**2)-(4*x)+3)\n",
        "\n",
        "def grad_slope(x):\n",
        "  return (2*x) - 4\n",
        "\n",
        "\n",
        "x = np.random.randint(100)\n",
        "print(\"Value of x:\",x)\n",
        "alpha = 0.1\n",
        "num_iters = 100\n",
        "x_values = []\n",
        "Improve_values = []\n",
        "tol = 0.1\n",
        "i=0\n",
        "\n",
        "while(True):\n",
        "  #cost = cost_func(x)\n",
        "  grad = grad_slope(x)\n",
        "\n",
        "  Improve_values.append(alpha * grad)\n",
        "  x = x - alpha * grad\n",
        "  x_values.append(x)\n",
        "  i+=1\n",
        "  if(alpha * grad < tol):\n",
        "    break\n",
        "\n",
        "print(x_values)\n",
        "print(Improve_values)\n",
        "print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "5i2nPhUCrzJ0"
      },
      "outputs": [],
      "source": [
        "## Gradient Descent Algo with Toelrance - Question 2 (Final)\n",
        "# Function to define the cost function\n",
        "def cost_function(x):\n",
        "  return ((x ** 2) - (4*x) + 3)\n",
        "\n",
        "#function to define the slope or first derivative\n",
        "def grad_slope(x):\n",
        "  return (2*x - 4)\n",
        "\n",
        "#function to find out gradient descent in iterative mode to keep moving towards the zero slope\n",
        "def gradient_descent(J, dJ, x, alpha, tol, max_iters):\n",
        "  #create a blank list and initialize with intial scalar parameter\n",
        "  List_x = [x]\n",
        "  List_Improvement = []\n",
        "\n",
        "  #iteration for indefinitely until we hit tolerance value\n",
        "  while(True):\n",
        "    #find the slope at initial scalar value x\n",
        "    dJ = grad_slope(x)\n",
        "\n",
        "    #subtracting value of x by multiplying its slope with learning rate in the direction to minimize the cost\n",
        "    x = x - alpha * dJ\n",
        "\n",
        "    #Recording improvement from the previous step\n",
        "    List_Improvement.append(alpha * dJ)\n",
        "    # Add updated value of x in the output list\n",
        "    List_x.append(x)\n",
        "\n",
        "    #Break the loop if the improvement is less than the tolerance value\n",
        "    if(alpha * grad < tol):\n",
        "      break\n",
        "\n",
        "  # Print all the value of x that shows the gradient descent towards the minima / zero slope\n",
        "  print(List_x)\n",
        "\n",
        "  x_final = x\n",
        "  J_history = List_x\n",
        "  return x_final,J_history"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
